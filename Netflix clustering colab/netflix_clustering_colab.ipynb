{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4b568aeb",
      "metadata": {
        "id": "4b568aeb"
      },
      "source": [
        "# Netflix Movies & TV Shows â€” Unsupervised Clustering\n",
        "**Ready-to-run Colab notebook**\n",
        "\n",
        "**What this notebook does**\n",
        "- Loads the Netflix dataset (2019 snapshot) from a Google Drive link.\n",
        "- Performs EDA, data cleaning, feature engineering (TF-IDF on descriptions + simple features).\n",
        "- Runs KMeans and Agglomerative clustering, evaluates with Silhouette score.\n",
        "- Visualizes clusters using PCA and saves cluster labels.\n",
        "\n",
        "**How to use**\n",
        "1. Open this notebook in Google Colab.\n",
        "2. Run cells from top to bottom.\n",
        "3. When prompted, the dataset will be downloaded using the provided Drive file id. If gdown fails, upload the CSV manually to the Colab session and update the path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1350581a",
      "metadata": {
        "id": "1350581a"
      },
      "outputs": [],
      "source": [
        "# Install required packages (only needed in Colab)\n",
        "!pip install -q scikit-learn pandas matplotlib seaborn gdown scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b854d6",
      "metadata": {
        "id": "f9b854d6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "sns.set(style='whitegrid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0632f733",
      "metadata": {
        "id": "0632f733"
      },
      "outputs": [],
      "source": [
        "# Download dataset from Google Drive using gdown.\n",
        "FILE_ID = '1xJGlInE12mAggLuRo8b0oNSshUIG8GvF'  # from your provided link\n",
        "output = 'netflix_titles.csv'\n",
        "\n",
        "try:\n",
        "    import gdown\n",
        "    url = f'https://drive.google.com/uc?id={FILE_ID}'\n",
        "    print('Downloading dataset...')\n",
        "    gdown.download(url, output, quiet=False)\n",
        "except Exception as e:\n",
        "    print('gdown failed or isn\\'t available.\\nError:', e)\n",
        "    print('If running in Colab, enable internet or upload the CSV manually to the session and set \"output\" to that path.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ffbe3a3",
      "metadata": {
        "id": "1ffbe3a3"
      },
      "outputs": [],
      "source": [
        "# Load dataset (ensure netflix_titles.csv is present in the working directory)\n",
        "try:\n",
        "    df = pd.read_csv('NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "    print('Loaded dataset with shape:', df.shape)\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"File netflix_titles.csv not found in the session. Please upload it or re-run the download cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde455f2",
      "metadata": {
        "id": "fde455f2"
      },
      "outputs": [],
      "source": [
        "# Basic dataset information\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "403b92d5",
      "metadata": {
        "id": "403b92d5"
      },
      "outputs": [],
      "source": [
        "# Missing values summary\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dffe064",
      "metadata": {
        "id": "8dffe064"
      },
      "outputs": [],
      "source": [
        "# Type distribution (Movie vs TV Show)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x='type')\n",
        "plt.title('Movies vs TV Shows')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dff15b0",
      "metadata": {
        "id": "5dff15b0"
      },
      "outputs": [],
      "source": [
        "# Top 15 countries by content (note: 'country' may have multiple countries per row)\n",
        "top_countries = (df['country'].dropna()\n",
        "                  .str.split(',').explode()\n",
        "                  .str.strip()\n",
        "                  .value_counts().head(15))\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=top_countries.values, y=top_countries.index)\n",
        "plt.title('Top 15 countries by content count')\n",
        "plt.xlabel('Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2cc517",
      "metadata": {
        "id": "9c2cc517"
      },
      "outputs": [],
      "source": [
        "# Releases over years\n",
        "plt.figure(figsize=(10,4))\n",
        "df['release_year'].value_counts().sort_index().plot(kind='line')\n",
        "plt.title('Content by Release Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8d1430",
      "metadata": {
        "id": "4f8d1430"
      },
      "outputs": [],
      "source": [
        "# Basic cleaning\n",
        "df = df.drop_duplicates(subset=['title', 'type', 'release_year'])\n",
        "df = df.dropna(subset=['title', 'description'])\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "print('After cleaning shape:', df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95cffcf",
      "metadata": {
        "id": "d95cffcf"
      },
      "outputs": [],
      "source": [
        "# Feature engineering\n",
        "df['type_encoded'] = df['type'].map({'Movie':0, 'TV Show':1})\n",
        "df['primary_genre'] = df['listed_in'].fillna('Unknown').apply(lambda x: x.split(',')[0].strip())\n",
        "df['desc'] = df['description'].fillna('')\n",
        "df[['title','type','type_encoded','primary_genre']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1ebc83",
      "metadata": {
        "id": "9d1ebc83"
      },
      "outputs": [],
      "source": [
        "# TF-IDF on description\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=3000)\n",
        "tfidf_matrix = tfidf.fit_transform(df['desc'])\n",
        "print('TF-IDF matrix shape:', tfidf_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f449fc5",
      "metadata": {
        "id": "5f449fc5"
      },
      "outputs": [],
      "source": [
        "# Numeric features to combine (scaled)\n",
        "num_feats = df[['type_encoded']].astype(float)\n",
        "scaler = StandardScaler()\n",
        "num_scaled = scaler.fit_transform(num_feats)\n",
        "from scipy.sparse import csr_matrix\n",
        "num_scaled_sparse = csr_matrix(num_scaled)\n",
        "\n",
        "# Combined feature matrix\n",
        "X = hstack([num_scaled_sparse, tfidf_matrix])\n",
        "print('Combined feature matrix shape:', X.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5feaafc5",
      "metadata": {
        "id": "5feaafc5"
      },
      "outputs": [],
      "source": [
        "# Helper function to run clustering and report silhouette\n",
        "def run_kmeans(X, k, random_state=42):\n",
        "    km = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
        "    labels = km.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    return labels, score, km\n",
        "\n",
        "def run_agglomerative(X, k):\n",
        "    agg = AgglomerativeClustering(n_clusters=k)\n",
        "    labels = agg.fit_predict(X.toarray()) if hasattr(X, 'toarray') else agg.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    return labels, score, agg\n",
        "\n",
        "# Try a range of k and pick best by silhouette (KMeans)\n",
        "scores = []\n",
        "ks = list(range(2,8))\n",
        "for k in ks:\n",
        "    labels, score, _ = run_kmeans(X, k)\n",
        "    scores.append(score)\n",
        "    print(f'KMeans k={k} -> silhouette={score:.4f}')\n",
        "\n",
        "best_k = ks[np.argmax(scores)]\n",
        "print('\\nBest k by silhouette (KMeans):', best_k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "347f64cc",
      "metadata": {
        "id": "347f64cc"
      },
      "outputs": [],
      "source": [
        "# Run KMeans with best_k and Agglomerative for comparison\n",
        "best_k = int(best_k)\n",
        "k_labels, k_score, k_model = run_kmeans(X, best_k)\n",
        "print('KMeans silhouette:', k_score)\n",
        "\n",
        "agg_labels, agg_score, agg_model = run_agglomerative(X, best_k)\n",
        "print('Agglomerative silhouette:', agg_score)\n",
        "\n",
        "# Attach labels to df (use KMeans labels by default)\n",
        "df['cluster_kmeans'] = k_labels\n",
        "df['cluster_agglo'] = agg_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92470a22",
      "metadata": {
        "id": "92470a22"
      },
      "outputs": [],
      "source": [
        "# PCA to 2D for visualization (use dense array for PCA)\n",
        "print('Converting features to dense array for PCA (may be memory heavy). If memory problems occur, reduce TF-IDF max_features.')\n",
        "X_dense = X.toarray()\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_dense)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=df['cluster_kmeans'].astype(str), palette='tab10', s=40)\n",
        "plt.title('KMeans clusters visualized with PCA (2 components)')\n",
        "plt.legend(title='cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd15f20b",
      "metadata": {
        "id": "fd15f20b"
      },
      "outputs": [],
      "source": [
        "# Inspect top genres and sample titles per cluster\n",
        "for c in sorted(df['cluster_kmeans'].unique()):\n",
        "    print('\\n=== Cluster', c, 'summary ===')\n",
        "    print('Count:', (df['cluster_kmeans']==c).sum())\n",
        "    print('Top primary genres:')\n",
        "    print(df[df['cluster_kmeans']==c]['primary_genre'].value_counts().head(5))\n",
        "    print('\\nSample titles:')\n",
        "    print(df[df['cluster_kmeans']==c].sample(min(5, (df['cluster_kmeans']==c).sum()))[['title','type','release_year']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6009e0d6",
      "metadata": {
        "id": "6009e0d6"
      },
      "outputs": [],
      "source": [
        "# Save clustered dataset to CSV\n",
        "out_file = 'netflix_titles_clustered.csv'\n",
        "df.to_csv(out_file, index=False)\n",
        "print('Saved clustered dataset to', out_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Conclusion & Next steps\n",
        "What I did\n",
        "\n",
        "Cleaned dataset, created TF-IDF features from descriptions, added a simple type feature.\n",
        "Ran KMeans and Agglomerative clustering and visualized clusters via PCA.\n",
        "Saved cluster assignments.\n",
        "\n",
        "Next steps / improvements\n",
        "\n",
        "Use more engineered features: cast, director, runtime, multiple-genre indicators.\n",
        "Use UMAP for better visualization with sparse inputs.\n",
        "Try topic modeling (LDA) on descriptions before clustering.\n",
        "Link clusters with IMDB ratings or user engagement metrics for business insights.\n"
      ],
      "metadata": {
        "id": "JOWpDxcaqrLL"
      },
      "id": "JOWpDxcaqrLL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}