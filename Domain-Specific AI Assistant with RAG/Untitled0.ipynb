{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKRDPAVU2PIN"
      },
      "outputs": [],
      "source": [
        "# === Colab-friendly RAG Week1-3 demo (paste & run in one cell) ===\n",
        "\n",
        "# 1) Install dependencies (tries to avoid common Colab conflicts)\n",
        "import sys, subprocess, os, json, textwrap\n",
        "\n",
        "def pip_install(packages):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--no-warn-script-location\"] + packages\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Try lightweight installs; sentence-transformers will bring torch automatically.\n",
        "try:\n",
        "    # faiss-cpu sometimes fails with latest builds; pin a compatible release if needed.\n",
        "    pip_install([\n",
        "        \"pdfplumber\",\n",
        "        \"python-docx\",\n",
        "        \"beautifulsoup4\",\n",
        "        \"sentence-transformers\",\n",
        "        \"faiss-cpu\",\n",
        "        \"openai\"\n",
        "    ])\n",
        "except subprocess.CalledProcessError as e:\n",
        "    # If faiss-cpu install fails, try a different common wheel\n",
        "    print(\"Primary install failed:\", e)\n",
        "    print(\"Attempting fallback install for faiss-cpu...\")\n",
        "    try:\n",
        "        pip_install([\"faiss-cpu==1.7.4\"])\n",
        "    except Exception as e2:\n",
        "        print(\"faiss-cpu fallback failed; you may need to install faiss manually.\")\n",
        "        # continue â€” index.py will raise helpful error if faiss missing\n",
        "\n",
        "# 2) Write helper files (utils.py, ingest.py, index.py, query_demo.py)\n",
        "utils_py = r'''\n",
        "import re, json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def save_jsonl(path: str, records: List[Dict[str, Any]]):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in records:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def load_jsonl(path: str):\n",
        "    out=[]\n",
        "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            out.append(json.loads(line))\n",
        "    return out\n",
        "\n",
        "def simple_chunk_text(text: str, chunk_size_chars: int = 2000, overlap_chars: int = 200):\n",
        "    text = text.replace('\\\\r\\\\n','\\\\n')\n",
        "    paragraphs = text.split('\\\\n\\\\n')\n",
        "    chunks = []\n",
        "    buffer=\"\"\n",
        "    for p in paragraphs:\n",
        "        if len(buffer) + len(p) + 2 <= chunk_size_chars:\n",
        "            buffer = (buffer + \"\\\\n\\\\n\" + p).strip()\n",
        "        else:\n",
        "            if buffer:\n",
        "                chunks.append(buffer.strip())\n",
        "            if len(p) > chunk_size_chars:\n",
        "                start=0\n",
        "                while start < len(p):\n",
        "                    part = p[start:start+chunk_size_chars]\n",
        "                    chunks.append(part.strip())\n",
        "                    start += chunk_size_chars - overlap_chars\n",
        "                buffer=\"\"\n",
        "            else:\n",
        "                buffer=p\n",
        "    if buffer:\n",
        "        chunks.append(buffer.strip())\n",
        "    stitched=[]\n",
        "    for i,c in enumerate(chunks):\n",
        "        if i==0:\n",
        "            stitched.append(c)\n",
        "        else:\n",
        "            prev=stitched[-1]\n",
        "            overlap = prev[-overlap_chars:] if len(prev)>overlap_chars else prev\n",
        "            stitched.append((overlap+\"\\\\n\"+c).strip())\n",
        "    return stitched\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = re.sub(r'\\\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def build_prompt(question: str, chunks: List[Dict[str, Any]], instructions: str = None) -> str:\n",
        "    system = instructions or (\n",
        "        \"You are a helpful domain expert. Answer using ONLY the provided document excerpts. \"\n",
        "        \"Cite sources inline using [DOC<n>]. If not supported, say 'I don't know'.\"\n",
        "    )\n",
        "    parts = [\"System: \" + system, \"\\\\nUser question: \" + question, \"\\\\nProvided documents:\"]\n",
        "    for i,ch in enumerate(chunks, start=1):\n",
        "        title = ch.get('metadata',{}).get('title') or f\"doc_{i}\"\n",
        "        meta = f\"Title: {title} | Source: {ch.get('metadata',{}).get('source','')}\"\n",
        "        parts.append(f\"[DOC{i}] {meta}\\\\n\" + ch['text'][:3000])\n",
        "    parts.append(\"\\\\nAnswer (include citations like [DOC1]):\")\n",
        "    return \"\\\\n\\\\n\".join(parts)\n",
        "'''\n",
        "\n",
        "ingest_py = r'''\n",
        "import os, uuid\n",
        "from utils import simple_chunk_text, normalize_text, save_jsonl\n",
        "\n",
        "def read_text_from_path(path: str) -> str:\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext == '.pdf':\n",
        "        try:\n",
        "            import pdfplumber\n",
        "        except:\n",
        "            raise RuntimeError(\"pdfplumber not installed\")\n",
        "        text=[]\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                p = page.extract_text()\n",
        "                if p:\n",
        "                    text.append(p)\n",
        "        return \"\\n\\n\".join(text)\n",
        "    elif ext in ['.docx','.doc']:\n",
        "        try:\n",
        "            import docx\n",
        "        except:\n",
        "            raise RuntimeError(\"python-docx not installed\")\n",
        "        doc = docx.Document(path)\n",
        "        paras = [p.text for p in doc.paragraphs if p.text and p.text.strip()]\n",
        "        return \"\\n\\n\".join(paras)\n",
        "    else:\n",
        "        with open(path,'r',encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "\n",
        "def ingest_file(path: str, title: str=None, chunk_size:int=2000, overlap:int=200):\n",
        "    text = read_text_from_path(path)\n",
        "    text = normalize_text(text)\n",
        "    chunks = simple_chunk_text(text, chunk_size_chars=chunk_size, overlap_chars=overlap)\n",
        "    records=[]\n",
        "    doc_id = str(uuid.uuid4())\n",
        "    for i,c in enumerate(chunks,start=1):\n",
        "        rec = {\n",
        "            'id': f\"{doc_id}_{i}\",\n",
        "            'doc_id': doc_id,\n",
        "            'text': c,\n",
        "            'metadata': {'title': title or os.path.basename(path), 'source': os.path.abspath(path), 'chunk_index': i}\n",
        "        }\n",
        "        records.append(rec)\n",
        "    return records\n",
        "\n",
        "if __name__=='__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input', '-i', required=True)\n",
        "    parser.add_argument('--output', '-o', default='chunks.jsonl')\n",
        "    args = parser.parse_args()\n",
        "    recs = ingest_file(args.input)\n",
        "    save_jsonl(args.output, recs)\n",
        "    print(f\"Saved {len(recs)} chunks to {args.output}\")\n",
        "'''\n",
        "\n",
        "index_py = r'''\n",
        "import os, pickle, numpy as np\n",
        "from utils import load_jsonl\n",
        "try:\n",
        "    import faiss\n",
        "except Exception:\n",
        "    faiss=None\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    SentenceTransformer=None\n",
        "\n",
        "try:\n",
        "    import openai\n",
        "except:\n",
        "    openai=None\n",
        "\n",
        "class Embedder:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', use_openai=False):\n",
        "        self.use_openai = use_openai and (openai is not None)\n",
        "        self.model_name = model_name\n",
        "        if not self.use_openai:\n",
        "            if SentenceTransformer is None:\n",
        "                raise RuntimeError(\"sentence-transformers not installed\")\n",
        "            self.model = SentenceTransformer(model_name)\n",
        "    def embed_texts(self, texts):\n",
        "        if self.use_openai:\n",
        "            emb=[]\n",
        "            for t in texts:\n",
        "                resp = openai.Embedding.create(model='text-embedding-3-small', input=t)\n",
        "                v = np.array(resp['data'][0]['embedding'], dtype='float32')\n",
        "                emb.append(v)\n",
        "            return np.vstack(emb)\n",
        "        else:\n",
        "            arr = self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "            if arr.dtype != np.float32:\n",
        "                arr = arr.astype('float32')\n",
        "            return arr\n",
        "\n",
        "class FaissIndexWrapper:\n",
        "    def __init__(self, dim, index_path='faiss.index', meta_path='meta.pkl'):\n",
        "        if faiss is None:\n",
        "            raise RuntimeError(\"faiss not installed\")\n",
        "        self.dim = dim\n",
        "        self.index = faiss.IndexFlatIP(dim)\n",
        "        self.index_path = index_path\n",
        "        self.meta_path = meta_path\n",
        "        self.metadata=[]\n",
        "    def add(self, vectors, metas):\n",
        "        faiss.normalize_L2(vectors)\n",
        "        self.index.add(vectors)\n",
        "        self.metadata.extend(metas)\n",
        "    def save(self):\n",
        "        faiss.write_index(self.index, self.index_path)\n",
        "        with open(self.meta_path,'wb') as f:\n",
        "            pickle.dump(self.metadata,f)\n",
        "    def load(self):\n",
        "        if os.path.exists(self.index_path):\n",
        "            self.index = faiss.read_index(self.index_path)\n",
        "        if os.path.exists(self.meta_path):\n",
        "            with open(self.meta_path,'rb') as f:\n",
        "                self.metadata = pickle.load(f)\n",
        "    def search(self, qvect, top_k=5):\n",
        "        faiss.normalize_L2(qvect)\n",
        "        D,I = self.index.search(qvect, top_k)\n",
        "        results=[]\n",
        "        for i_list, d_list in zip(I.tolist(), D.tolist()):\n",
        "            row=[]\n",
        "            for idx, score in zip(i_list, d_list):\n",
        "                if idx < 0 or idx >= len(self.metadata): continue\n",
        "                meta = self.metadata[idx].copy()\n",
        "                meta['_score']=float(score)\n",
        "                row.append(meta)\n",
        "            results.append(row)\n",
        "        return results\n",
        "\n",
        "if __name__=='__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--chunks', required=True)\n",
        "    parser.add_argument('--index_path', default='faiss.index')\n",
        "    parser.add_argument('--meta_path', default='meta.pkl')\n",
        "    parser.add_argument('--use_openai', action='store_true')\n",
        "    parser.add_argument('--model', default='all-MiniLM-L6-v2')\n",
        "    args = parser.parse_args()\n",
        "    recs = load_jsonl(args.chunks)\n",
        "    texts = [r['text'] for r in recs]\n",
        "    metas = [{'id':r['id'],'doc_id':r['doc_id'],'metadata':r.get('metadata',{}),'text':r['text']} for r in recs]\n",
        "    embedder = Embedder(model_name=args.model, use_openai=args.use_openai)\n",
        "    embs = embedder.embed_texts(texts)\n",
        "    dim = embs.shape[1]\n",
        "    wrapper = FaissIndexWrapper(dim=dim, index_path=args.index_path, meta_path=args.meta_path)\n",
        "    wrapper.add(embs, metas)\n",
        "    wrapper.save()\n",
        "    print(f\"Indexed {len(texts)} chunks\")\n",
        "'''\n",
        "\n",
        "query_demo_py = r'''\n",
        "import os, json\n",
        "from utils import build_prompt, load_jsonl\n",
        "from index import Embedder, FaissIndexWrapper\n",
        "\n",
        "def run_query(question, top_k=5, use_openai_emb=False, openai_key=None, gen_with_openai=False, gen_model='gpt-4o-mini'):\n",
        "    if openai_key:\n",
        "        import openai\n",
        "        openai.api_key = openai_key\n",
        "    embedder = Embedder(use_openai=use_openai_emb)\n",
        "    dim = embedder.model.get_sentence_embedding_dimension() if not use_openai_emb else 1536\n",
        "    wrapper = FaissIndexWrapper(dim=dim)\n",
        "    try:\n",
        "        wrapper.load()\n",
        "    except Exception as e:\n",
        "        print(\"Failed to load index:\", e)\n",
        "        return\n",
        "    q_emb = embedder.embed_texts([question])\n",
        "    results = wrapper.search(q_emb, top_k=top_k)[0]\n",
        "    chunks = [{'id':h['id'],'text':h['text'],'metadata':h.get('metadata',{})} for h in results]\n",
        "    prompt = build_prompt(question, chunks)\n",
        "    if gen_with_openai and openai_key:\n",
        "        import openai\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=gen_model,\n",
        "            messages=[{\"role\":\"system\",\"content\":prompt}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=512\n",
        "        )\n",
        "        ans = resp['choices'][0]['message']['content'].strip()\n",
        "    else:\n",
        "        ans = \"[No generation configured] Prompt below:\\\\n\\\\n\" + prompt\n",
        "    return {'answer': ans, 'sources': [{'id':h['id'],'title':h.get('metadata',{}).get('title'),'_score':h.get('_score')} for h in results]}\n",
        "\n",
        "if __name__=='__main__':\n",
        "    res = run_query(\"What is the main topic of the document?\", top_k=3)\n",
        "    print(json.dumps(res, indent=2))\n",
        "'''\n",
        "\n",
        "# write files\n",
        "open('utils.py', 'w', encoding='utf-8').write(utils_py)\n",
        "open('ingest.py', 'w', encoding='utf-8').write(ingest_py)\n",
        "open('index.py', 'w', encoding='utf-8').write(index_py)\n",
        "open('query_demo.py', 'w', encoding='utf-8').write(query_demo_py)\n",
        "print(\"Wrote utils.py, ingest.py, index.py, query_demo.py\")\n",
        "\n",
        "# 3) Create a sample document\n",
        "sample_text = textwrap.dedent(\"\"\"\n",
        "Title: Sample Policy Document\n",
        "\n",
        "Section 1: Introduction\n",
        "This document explains the sample policy for demonstration purposes. The policy covers acceptable usage, privacy, and responsibilities.\n",
        "\n",
        "Section 2: Acceptable Usage\n",
        "Users must follow the rules. Do not attempt to circumvent restrictions. Use the platform for lawful purposes only.\n",
        "\n",
        "Section 3: Privacy\n",
        "Personal data must be handled carefully. Sensitive data should be redacted and stored securely.\n",
        "\n",
        "Section 4: Responsibilities\n",
        "Administrators manage users and content. Users are responsible for their own actions. Contact admin@example.com for support.\n",
        "\"\"\").strip()\n",
        "\n",
        "with open('sample_doc.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(sample_text)\n",
        "print(\"Wrote sample_doc.txt\")\n",
        "\n",
        "# 4) Ingest the sample doc to create chunks.jsonl\n",
        "print(\"\\n=== Running ingestion ===\")\n",
        "!python ingest.py --input sample_doc.txt --output chunks.jsonl\n",
        "\n",
        "# show chunks\n",
        "print(\"\\n--- chunks.jsonl preview ---\")\n",
        "!sed -n '1,200p' chunks.jsonl || true\n",
        "\n",
        "# 5) Build embeddings and FAISS index (local sentence-transformers by default)\n",
        "print(\"\\n=== Building index (this may download models; allow a few minutes) ===\")\n",
        "!python index.py --chunks chunks.jsonl --index_path faiss.index --meta_path meta.pkl --model all-MiniLM-L6-v2\n",
        "\n",
        "# 6) Optional: set OPENAI_API_KEY if you want to use OpenAI for embeddings/generation\n",
        "print(\"\\nIf you want OpenAI usage, set environment variable OPENAI_API_KEY before running the query step.\")\n",
        "print(\"You can do: os.environ['OPENAI_API_KEY'] = 'sk-...' in a new cell.\")\n",
        "\n",
        "# 7) Run a demo query (no OpenAI by default -> prints the prompt)\n",
        "print(\"\\n=== Running demo query ===\")\n",
        "from query_demo import run_query\n",
        "res = run_query(\"What does the document say about privacy?\", top_k=3,\n",
        "                use_openai_emb=False,\n",
        "                openai_key=os.environ.get('OPENAI_API_KEY'),\n",
        "                gen_with_openai=False,\n",
        "                gen_model='gpt-4o-mini')\n",
        "print(json.dumps(res, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Colab: Week4-5 Extension (Reranker + Extractive QA + Eval) ===\n",
        "# Paste & run in the same Colab session where Week1-3 files exist.\n",
        "\n",
        "import sys, subprocess, os, json, textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- 1) Install any missing packages ----------\n",
        "def pip_install(packages):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--no-warn-script-location\"] + packages\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "try:\n",
        "    pip_install([\"cross-encoder\", \"transformers\", \"nltk\"])\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"Install failed:\", e)\n",
        "    # proceed; some packages might already exist\n",
        "\n",
        "# Download punkt and punkt_tab if needed\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except LookupError:\n",
        "    nltk.download(\"punkt\")\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
        "except LookupError:\n",
        "    # Try downloading punkt_tab explicitly\n",
        "    nltk.downloader.download('punkt_tab')\n",
        "\n",
        "\n",
        "# ---------- 2) Write reranker + qa utility module (rerank_and_qa.py) ----------\n",
        "rerank_and_qa = r'''\n",
        "import math\n",
        "import nltk\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "# Cross-encoder reranker (sentence-transformers cross-encoder)\n",
        "try:\n",
        "    from sentence_transformers import CrossEncoder\n",
        "except Exception as e:\n",
        "    CrossEncoder = None\n",
        "    print(\"CrossEncoder import failed:\", e)\n",
        "\n",
        "_cross_encoder_cache = {}\n",
        "\n",
        "def get_cross_encoder(model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
        "    if model_name in _cross_encoder_cache:\n",
        "        return _cross_encoder_cache[model_name]\n",
        "    if CrossEncoder is None:\n",
        "        raise RuntimeError(\"CrossEncoder not available. Install sentence-transformers.\")\n",
        "    model = CrossEncoder(model_name)\n",
        "    _cross_encoder_cache[model_name] = model\n",
        "    return model\n",
        "\n",
        "def rerank(question: str, candidates: List[Dict[str, Any]], top_n: int = 5, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    candidates: list of dicts with keys: 'id','text','metadata'\n",
        "    Returns top_n candidates sorted by cross-encoder score (higher is better).\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "    model = get_cross_encoder(model_name)\n",
        "    # Prepare inputs: pairs of (question, candidate_text)\n",
        "    texts = [c['text'] for c in candidates]\n",
        "    inputs = [[question, t] for t in texts]\n",
        "    scores = model.predict(inputs)  # shape (len(candidates),)\n",
        "    # attach scores and sort\n",
        "    out = []\n",
        "    for c, s in zip(candidates, scores):\n",
        "        cc = c.copy()\n",
        "        cc['_rerank_score'] = float(s)\n",
        "        out.append(cc)\n",
        "    out.sort(key=lambda x: x['_rerank_score'], reverse=True)\n",
        "    return out[:top_n]\n",
        "\n",
        "# -------- Extractive QA fallback (simple sentence-level selection) --------\n",
        "def extractive_answer(question: str, top_chunks: List[Dict[str, Any]], top_k_sentences:int = 2) -> Tuple[str, List[Dict[str,Any]]]:\n",
        "    \"\"\"\n",
        "    Simple extractive approach:\n",
        "      - Split each chunk into sentences\n",
        "      - Score sentences by overlap of important words (very simple)\n",
        "      - Return top sentences concatenated as the extractive answer\n",
        "    Returns: (answer_text, list_of_evidence_dicts)\n",
        "    evidence_dict: {'doc_id','chunk_id','sentence','score','citation'}\n",
        "    \"\"\"\n",
        "    import re\n",
        "    from collections import Counter\n",
        "    # basic tokenization\n",
        "    def tokenize(s):\n",
        "        return [t.lower() for t in re.findall(r\"\\\\w+\", s)]\n",
        "    q_tokens = tokenize(question)\n",
        "    q_counter = Counter(q_tokens)\n",
        "\n",
        "    evidences = []\n",
        "    for chunk in top_chunks:\n",
        "        sents = nltk.sent_tokenize(chunk['text'])\n",
        "        for sent in sents:\n",
        "            tokens = tokenize(sent)\n",
        "            # score: sum of counts of tokens appearing in question, weighted by token length\n",
        "            score = sum(q_counter.get(t,0) for t in tokens)\n",
        "            # also prefer longer sentences if score equal\n",
        "            score = score + 0.001 * len(tokens)\n",
        "            evidences.append({\n",
        "                'doc_id': chunk.get('doc_id'),\n",
        "                'chunk_id': chunk.get('id'),\n",
        "                'sentence': sent,\n",
        "                'score': float(score),\n",
        "                'metadata': chunk.get('metadata',{})\n",
        "            })\n",
        "    # sort evidences\n",
        "    evidences.sort(key=lambda x: x['score'], reverse=True)\n",
        "    if not evidences:\n",
        "        return (\"I couldn't find relevant sentences in the provided documents.\", [])\n",
        "    top = evidences[:top_k_sentences]\n",
        "    answer = \" \".join([t['sentence'] for t in top])\n",
        "    # add citation labels like [DOCn] using metadata title\n",
        "    for i, t in enumerate(top, start=1):\n",
        "        title = t.get('metadata',{}).get('title') or t.get('doc_id')\n",
        "        t['citation'] = f\"[{title}]\"\n",
        "    return (answer, top)\n",
        "'''\n",
        "\n",
        "open('rerank_and_qa.py', 'w', encoding='utf-8').write(rerank_and_qa)\n",
        "print(\"Wrote rerank_and_qa.py\")\n",
        "\n",
        "# ---------- 3) Update/Write an improved query runner (query_demo_v2.py) ----------\n",
        "query_demo_v2 = r'''\n",
        "import os, json\n",
        "from utils import build_prompt, load_jsonl\n",
        "from index import Embedder, FaissIndexWrapper\n",
        "from rerank_and_qa import rerank, extractive_answer\n",
        "\n",
        "def run_query_v2(question, top_k=10, rerank_top_n=5, use_openai_emb=False, openai_key=None, gen_with_openai=False, gen_model='gpt-4o-mini'):\n",
        "    \"\"\"\n",
        "    Steps:\n",
        "    1. Embed question\n",
        "    2. Retrieve top_k from FAISS\n",
        "    3. Rerank top_k using cross-encoder -> top rerank_top_n\n",
        "    4. If gen_with_openai True and key provided -> build prompt and call OpenAI\n",
        "       Else -> run extractive_answer on top reranked chunks and return extractive result with citations\n",
        "    \"\"\"\n",
        "    if openai_key:\n",
        "        try:\n",
        "            import openai\n",
        "            openai.api_key = openai_key\n",
        "        except Exception as e:\n",
        "            print(\"OpenAI import failed:\", e)\n",
        "            openai = None\n",
        "\n",
        "    # 1. embed + 2. retrieve\n",
        "    embedder = Embedder(use_openai=use_openai_emb)\n",
        "    dim = embedder.model.get_sentence_embedding_dimension() if not use_openai_emb else 1536\n",
        "    wrapper = FaissIndexWrapper(dim=dim)\n",
        "    try:\n",
        "        wrapper.load()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load index: {e}\")\n",
        "    q_emb = embedder.embed_texts([question])\n",
        "    raw_results = wrapper.search(q_emb, top_k)\n",
        "    hits = raw_results[0] if raw_results else []\n",
        "    # convert to candidate format expected by reranker\n",
        "    candidates = [{'id': h['id'], 'text': h['text'], 'metadata': h.get('metadata',{}), 'doc_id': h.get('doc_id')} for h in hits]\n",
        "    if not candidates:\n",
        "        return {'answer': \"No retrieved candidates.\", 'sources': [], 'reranked': []}\n",
        "\n",
        "    # 3. rerank\n",
        "    try:\n",
        "        reranked = rerank(question, candidates, top_n=rerank_top_n)\n",
        "    except Exception as e:\n",
        "        # if cross-encoder unavailable, fallback to original order\n",
        "        print(\"Rerank failed, falling back to initial order:\", e)\n",
        "        reranked = candidates[:rerank_top_n]\n",
        "\n",
        "    # 4. generate or extractive fallback\n",
        "    if gen_with_openai and openai_key:\n",
        "        # build prompt from top reranked\n",
        "        prompt_chunks = [{'id': c['id'], 'text': c['text'], 'metadata': c.get('metadata',{})} for c in reranked]\n",
        "        prompt = build_prompt(question, prompt_chunks)\n",
        "        import openai\n",
        "        resp = openai.ChatCompletion.create(\n",
        "            model=gen_model,\n",
        "            messages=[{\"role\":\"system\",\"content\":prompt}],\n",
        "            max_tokens=512,\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        answer = resp['choices'][0]['message']['content'].strip()\n",
        "        sources = [{'id': c['id'], 'title': c.get('metadata',{}).get('title'), '_rerank_score': c.get('_rerank_score')} for c in reranked]\n",
        "        return {'answer': answer, 'sources': sources, 'reranked': reranked}\n",
        "    else:\n",
        "        # extractive fallback\n",
        "        answer_text, evidence = extractive_answer(question, reranked, top_k_sentences=2)\n",
        "        # format sources\n",
        "        sources = []\n",
        "        for c in reranked:\n",
        "            sources.append({'id': c['id'], 'title': c.get('metadata',{}).get('title'), '_rerank_score': c.get('_rerank_score')})\n",
        "        return {'answer': answer_text, 'evidence': evidence, 'sources': sources, 'reranked': reranked}\n",
        "'''\n",
        "\n",
        "open('query_demo_v2.py', 'w', encoding='utf-8').write(query_demo_v2)\n",
        "print(\"Wrote query_demo_v2.py\")\n",
        "\n",
        "# ---------- 4) Write a small evaluation script (eval_recall.py) ----------\n",
        "eval_recall = r'''\n",
        "\"\"\"\n",
        "Simple evaluation script:\n",
        "- Maintains a tiny QA testset (list of dicts: question, gold_doc_title)\n",
        "- Computes Recall@k: whether gold doc title appears in top-k retrieved docs\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from index import Embedder, FaissIndexWrapper\n",
        "from utils import load_jsonl\n",
        "\n",
        "def recall_at_k(testset, k=5, use_openai_emb=False):\n",
        "    embedder = Embedder(use_openai=use_openai_emb)\n",
        "    dim = embedder.model.get_sentence_embedding_dimension() if not use_openai_emb else 1536\n",
        "    wrapper = FaissIndexWrapper(dim=dim)\n",
        "    wrapper.load()\n",
        "    total = len(testset)\n",
        "    hits = 0\n",
        "    for t in testset:\n",
        "        q = t['question']\n",
        "        gold = t['gold_title']\n",
        "        q_emb = embedder.embed_texts([q])\n",
        "        res = wrapper.search(q_emb, k)[0]\n",
        "        titles = [r.get('metadata',{}).get('title') for r in res]\n",
        "        if gold in titles:\n",
        "            hits += 1\n",
        "    return hits / total if total>0 else 0.0\n",
        "\n",
        "if __name__=='__main__':\n",
        "    # example small testset; extend with your own mappings to real documents\n",
        "    testset = [\n",
        "        {\"question\": \"What does the document say about privacy?\", \"gold_title\": \"sample_doc.txt\"},\n",
        "        {\"question\": \"Who manages users and content?\", \"gold_title\": \"sample_doc.txt\"},\n",
        "        {\"question\": \"What are acceptable uses?\", \"gold_title\": \"sample_doc.txt\"}\n",
        "    ]\n",
        "    for k in [1,3,5]:\n",
        "        r = recall_at_k(testset, k=k)\n",
        "        print(f\"Recall@{k}: {r:.3f}\")\n",
        "'''\n",
        "\n",
        "open('eval_recall.py', 'w', encoding='utf-8').write(eval_recall)\n",
        "print(\"Wrote eval_recall.py\")\n",
        "\n",
        "# ---------- 5) Demo run: use query_demo_v2 to show rerank + extractive QA ----------\n",
        "print(\"\\n=== Demo: run query_demo_v2 (rerank + extractive QA fallback) ===\")\n",
        "from query_demo_v2 import run_query_v2\n",
        "res = run_query_v2(\"What does the document say about privacy?\", top_k=5, rerank_top_n=3, use_openai_emb=False, openai_key=os.environ.get('OPENAI_API_KEY'), gen_with_openai=False)\n",
        "print(json.dumps(res, indent=2, ensure_ascii=False))\n",
        "\n",
        "# ---------- 6) Run small recall evaluation ----------\n",
        "print(\"\\n=== Running small Recall@k evaluation (eval_recall.py) ===\")\n",
        "!python eval_recall.py\n",
        "\n",
        "print(\"\\nDone. You now have:\")\n",
        "print(\"- rerank_and_qa.py : cross-encoder reranker + extractive QA\")\n",
        "print(\"- query_demo_v2.py : end-to-end query runner with rerank + optional OpenAI generation\")\n",
        "print(\"- eval_recall.py   : small Recall@k evaluator\\n\")\n",
        "print(\"Next suggestions:\")\n",
        "print(\"- If you have an OpenAI key and want model generation, set os.environ['OPENAI_API_KEY'] and call run_query_v2 with gen_with_openai=True.\")\n",
        "print(\"- Expand the small testset in eval_recall.py with real questions + gold doc titles to measure retrieval quality.\")"
      ],
      "metadata": {
        "id": "-pk-fGFB74jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Colab cell: Week 6â€“8 (FastAPI v2, eval_full, Dockerfile, docker-compose) ===\n",
        "import os, textwrap, json, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "\n",
        "# ---------- 1) Ensure required packages ----------\n",
        "def pip_install(packages):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--no-warn-script-location\"] + packages\n",
        "    print(\"Installing:\", \" \".join(packages))\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Install failed for some packages:\", e)\n",
        "\n",
        "pip_install([\n",
        "    \"fastapi\", \"uvicorn[standard]\", \"python-multipart\",\n",
        "    \"pydantic\", \"aiofiles\", \"cryptography\", \"pyjwt\", \"requests\"\n",
        "])\n",
        "\n",
        "# ---------- 2) rag_service_v2.py ----------\n",
        "rag_service_v2 = r'''\n",
        "# rag_service_v2.py\n",
        "import os, threading, tempfile, uuid\n",
        "from typing import List\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from pathlib import Path\n",
        "\n",
        "from ingest import ingest_file\n",
        "from index import Embedder, FaissIndexWrapper\n",
        "from utils import build_prompt\n",
        "from rerank_and_qa import rerank, extractive_answer\n",
        "\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "try:\n",
        "    import openai\n",
        "    if OPENAI_API_KEY:\n",
        "        openai.api_key = OPENAI_API_KEY\n",
        "except Exception:\n",
        "    openai = None\n",
        "\n",
        "INDEX_PATH = os.environ.get(\"INDEX_PATH\", \"faiss.index\")\n",
        "META_PATH = os.environ.get(\"META_PATH\", \"meta.pkl\")\n",
        "EMBED_MODEL = os.environ.get(\"EMBED_MODEL\", \"all-MiniLM-L6-v2\")\n",
        "USE_OPENAI_EMB = os.environ.get(\"USE_OPENAI_EMB\", \"false\").lower() in (\"1\",\"true\",\"yes\")\n",
        "\n",
        "app = FastAPI(title=\"RAG Assistant v2\")\n",
        "\n",
        "embedder = Embedder(model_name=EMBED_MODEL, use_openai=USE_OPENAI_EMB)\n",
        "dim = embedder.model.get_sentence_embedding_dimension() if not USE_OPENAI_EMB else 1536\n",
        "wrapper = FaissIndexWrapper(dim=dim, index_path=INDEX_PATH, meta_path=META_PATH)\n",
        "try:\n",
        "    wrapper.load()\n",
        "    print(\"Loaded existing FAISS index.\")\n",
        "except Exception:\n",
        "    print(\"No existing index loaded; starting empty.\")\n",
        "\n",
        "_index_lock = threading.Lock()\n",
        "\n",
        "def index_chunks(records):\n",
        "    texts = [r['text'] for r in records]\n",
        "    metas = [{'id': r['id'], 'doc_id': r.get('doc_id'),\n",
        "              'metadata': r.get('metadata', {}), 'text': r['text']} for r in records]\n",
        "    embs = embedder.embed_texts(texts)\n",
        "    with _index_lock:\n",
        "        wrapper.add(embs, metas)\n",
        "        wrapper.save()\n",
        "    return len(records)\n",
        "\n",
        "@app.post(\"/upload\")\n",
        "async def upload(files: List[UploadFile] = File(...)):\n",
        "    if not files:\n",
        "        raise HTTPException(status_code=400, detail=\"No files provided\")\n",
        "    all_records = []\n",
        "    for up in files:\n",
        "        suffix = Path(up.filename).suffix or \".txt\"\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
        "            content = await up.read()\n",
        "            tmp.write(content)\n",
        "            tmp.flush()\n",
        "            try:\n",
        "                recs = ingest_file(tmp.name, title=up.filename)\n",
        "                all_records.extend(recs)\n",
        "            finally:\n",
        "                try: os.unlink(tmp.name)\n",
        "                except: pass\n",
        "    if not all_records:\n",
        "        raise HTTPException(status_code=400, detail=\"No chunks created\")\n",
        "    job_id = uuid.uuid4().hex\n",
        "    thread = threading.Thread(target=index_chunks, args=(all_records,), daemon=True)\n",
        "    thread.start()\n",
        "    return {\"status\": \"accepted\", \"job_id\": job_id, \"chunks\": len(all_records)}\n",
        "\n",
        "class QueryIn(BaseModel):\n",
        "    question: str\n",
        "    top_k: int = 10\n",
        "    rerank_top_n: int = 5\n",
        "    gen: bool = False\n",
        "\n",
        "@app.post(\"/query\")\n",
        "def query(q: QueryIn):\n",
        "    q_text = q.question.strip()\n",
        "    if not q_text:\n",
        "        raise HTTPException(status_code=400, detail=\"question required\")\n",
        "    q_emb = embedder.embed_texts([q_text])\n",
        "    with _index_lock:\n",
        "        results = wrapper.search(q_emb, top_k=q.top_k)\n",
        "    hits = results[0] if results else []\n",
        "    candidates = [{'id': h['id'], 'text': h['text'],\n",
        "                   'metadata': h.get('metadata', {}), 'doc_id': h.get('doc_id')} for h in hits]\n",
        "    if not candidates:\n",
        "        return {\"answer\": \"No documents indexed.\", \"sources\": []}\n",
        "    try:\n",
        "        reranked = rerank(q_text, candidates, top_n=q.rerank_top_n)\n",
        "    except Exception:\n",
        "        reranked = candidates[:q.rerank_top_n]\n",
        "    if q.gen and openai is not None and OPENAI_API_KEY:\n",
        "        prompt_chunks = [{'id': c['id'], 'text': c['text'], 'metadata': c.get('metadata', {})} for c in reranked]\n",
        "        prompt = build_prompt(q_text, prompt_chunks)\n",
        "        try:\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model=os.environ.get(\"GEN_MODEL\", \"gpt-4o-mini\"),\n",
        "                messages=[{\"role\":\"system\", \"content\": prompt}],\n",
        "                max_tokens=512,\n",
        "                temperature=0.0,\n",
        "            )\n",
        "            answer = resp['choices'][0]['message']['content'].strip()\n",
        "        except Exception as e:\n",
        "            answer = f\"[Generation failed: {e}]\"\n",
        "        sources = [{'id': c['id'], 'title': c.get('metadata', {}).get('title'),\n",
        "                    '_rerank_score': c.get('_rerank_score')} for c in reranked]\n",
        "        return {\"answer\": answer, \"sources\": sources}\n",
        "    else:\n",
        "        answer_text, evidence = extractive_answer(q_text, reranked, top_k_sentences=2)\n",
        "        sources = [{'id': c['id'], 'title': c.get('metadata', {}).get('title'),\n",
        "                    '_rerank_score': c.get('_rerank_score')} for c in reranked]\n",
        "        return {\"answer\": answer_text, \"evidence\": evidence, \"sources\": sources}\n",
        "\n",
        "@app.post(\"/delete-index\")\n",
        "def delete_index():\n",
        "    try:\n",
        "        if os.path.exists(wrapper.index_path):\n",
        "            os.remove(wrapper.index_path)\n",
        "        if os.path.exists(wrapper.meta_path):\n",
        "            os.remove(wrapper.meta_path)\n",
        "        wrapper.metadata = []\n",
        "        import faiss\n",
        "        wrapper.index = faiss.IndexFlatIP(wrapper.dim)\n",
        "        return {\"status\": \"deleted\"}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\", \"indexed_docs\": len(wrapper.metadata)}\n",
        "'''\n",
        "open(\"rag_service_v2.py\",\"w\").write(rag_service_v2)\n",
        "\n",
        "# ---------- 3) eval_full.py (corrected) ----------\n",
        "eval_full = r'''\n",
        "# eval_full.py (corrected)\n",
        "import json\n",
        "from query_demo_v2 import run_query_v2\n",
        "\n",
        "def main():\n",
        "    questions = [\n",
        "        \"What does the document say about privacy?\",\n",
        "        \"Who manages users and content?\",\n",
        "        \"What are acceptable uses?\"\n",
        "    ]\n",
        "    print(\"Running sanity-check queries:\\n\")\n",
        "    for q in questions:\n",
        "        try:\n",
        "            res = run_query_v2(q, top_k=5, rerank_top_n=3,\n",
        "                               use_openai_emb=False,\n",
        "                               openai_key=None,\n",
        "                               gen_with_openai=False)\n",
        "            print(\"Q:\", q)\n",
        "            print(\"Answer:\", res.get(\"answer\"))\n",
        "            if res.get(\"evidence\"):\n",
        "                print(\"Evidence:\")\n",
        "                for e in res[\"evidence\"]:\n",
        "                    title = e.get(\"metadata\",{}).get(\"title\") or e.get(\"doc_id\")\n",
        "                    print(f\" - {title}: {e.get('sentence')[:120]} (score={e.get('score')})\")\n",
        "            print(\"Sources:\", [s.get(\"title\") for s in res.get(\"sources\", [])])\n",
        "            print(\"-\"*60)\n",
        "        except Exception as ex:\n",
        "            print(\"Error on query:\", q, ex)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n",
        "'''\n",
        "open(\"eval_full.py\",\"w\").write(eval_full)\n",
        "\n",
        "# ---------- 4) Dockerfile ----------\n",
        "dockerfile = r'''\n",
        "FROM python:3.11-slim\n",
        "WORKDIR /app\n",
        "ENV PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1\n",
        "RUN apt-get update && apt-get install -y build-essential git curl && rm -rf /var/lib/apt/lists/*\n",
        "COPY requirements-runtime.txt /app/requirements-runtime.txt\n",
        "RUN pip install --no-cache-dir -r requirements-runtime.txt\n",
        "COPY . /app\n",
        "EXPOSE 8000\n",
        "CMD [\"uvicorn\",\"rag_service_v2:app\",\"--host\",\"0.0.0.0\",\"--port\",\"8000\"]\n",
        "'''\n",
        "open(\"Dockerfile\",\"w\").write(dockerfile)\n",
        "\n",
        "# ---------- 5) requirements-runtime.txt ----------\n",
        "requirements_runtime = \"\\n\".join([\n",
        "    \"fastapi\",\n",
        "    \"uvicorn[standard]\",\n",
        "    \"python-multipart\",\n",
        "    \"sentence-transformers\",\n",
        "    \"faiss-cpu\",\n",
        "    \"pdfplumber\",\n",
        "    \"python-docx\",\n",
        "    \"beautifulsoup4\",\n",
        "    \"openai\",\n",
        "    \"pydantic\",\n",
        "    \"aiofiles\"\n",
        "])\n",
        "open(\"requirements-runtime.txt\",\"w\").write(requirements_runtime)\n",
        "\n",
        "# ---------- 6) docker-compose.yml ----------\n",
        "docker_compose = r'''\n",
        "version: '3.8'\n",
        "services:\n",
        "  rag:\n",
        "    build: .\n",
        "    ports:\n",
        "      - \"8000:8000\"\n",
        "    volumes:\n",
        "      - .:/app\n",
        "    environment:\n",
        "      - OPENAI_API_KEY=${OPENAI_API_KEY:-}\n",
        "      - EMBED_MODEL=all-MiniLM-L6-v2\n",
        "      - INDEX_PATH=faiss.index\n",
        "      - META_PATH=meta.pkl\n",
        "'''\n",
        "open(\"docker-compose.yml\",\"w\").write(docker_compose)\n",
        "\n",
        "print(\"âœ… Wrote rag_service_v2.py, eval_full.py, Dockerfile, requirements-runtime.txt, docker-compose.yml\")\n",
        "\n",
        "# ---------- 7) Run sanity check ----------\n",
        "print(\"\\n=== Running eval_full.py ===\")\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"eval_full.py\"])\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"Sanity-check failed:\", e)\n",
        "\n",
        "print(\"\\nAll Week 6â€“8 files are ready. Next steps:\")\n",
        "print(\"- Run `uvicorn rag_service_v2:app --reload --port 8000` locally to start the API.\")\n",
        "print(\"- Or build with Docker: `docker build -t rag-assistant .` then `docker run -p 8000:8000 rag-assistant`.\")\n",
        "print(\"- Or use docker-compose: `docker-compose up --build`.\")\n"
      ],
      "metadata": {
        "id": "zy_BBRhBuMqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the FastAPI server\n",
        "get_ipython().system('uvicorn rag_service_v2:app --reload --port 8000')"
      ],
      "metadata": {
        "id": "KX-ijMFAzY5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}